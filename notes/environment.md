# Environment overview


**NOTE:** two primary systems used to run the code were Windows 10 Home with Desbian WSL and an old Xeon compute server.
As such the notes are based around those systems.

## Execution environment setup

The majority of the dependancies can be installed by simply installing `Python 3.7`, 
initializing a viritual environment, then running `pip install -r requirements.txt`.


### FastText

`FastText` was one of the packages that requires the most steps to install.
The following steps were taken to install it:

1. Clone the [repo](https://github.com/facebookresearch/fastText)
2. Build and install the project (as specified [here](https://github.com/facebookresearch/fastText/tree/master/python))
    1. **NOTE:** This required the installation of [VS C++ build tools](https://visualstudio.microsoft.com/downloads/#build-tools-for-visual-studio-2017)
    2. When building ensure you are in your projects virtual environment, if you're using PyCharm you can simply use the 
    *Terminal* tab at the bottom to navigate into the fastText directory in order to build it.
3. Download the English pre-trained [model](https://fasttext.cc/docs/en/crawl-vectors.html) (`.bin`)
    1. **NOTE:** The simple `.txt` model can be used if you don't have any out-of-vocabulary (OOV) words

#### Custom FastText Embeddings

To generate custom FastText embeddings for a given corpus simply do the following:

1. Follow the steps above to install FastText
2. Pre-process the desired dataset
3. Copy **only the document content** to the fast text directory (optional, can also reference the file)
    * This can be done by running [`fast_text_prep.py`](../execution/pre_train/fast_text_prep.py) 
    (NOTE: ensure the FastText installation location is the same, assumed to be in the same directory as this project)
4. Run `./fasttext skipgram -input "data/$1" -output "data/$2" -dim 300 -thread NUM_THREADS` in the FastText directory
    * `$1` is the source filename (ex. `mixed_dataset.csv`)
    * `$2` is the destination name (ex. `embeddings`) (NOTE: no file extension)
    * `-dim 300` will give you embeddings with a dimension of 300 (same as pre-trained), can be changed
4. Move the `.bin` file into `data/models/` and change the FastText model in [`config.json`](../config.json)
    
### SpaCy

SpaCy will be installed by executing the `pip install` listed above, however the chosen model will still have to be downloaded.
By default the one used in the code is the `en_core_web_sm` model (small english model).
It can be installed by opening the shell/command prompt, entering the virtual environment, then running `python -m spacy download en_core_web_sm`.

### TensorFlow

For some systems the pre-built binaries will not work, so you cannot simply use `pip install tensorflow`.
In these cases you can use [these](https://github.com/fo40225/tensorflow-windows-wheel) pre-built ones. 
Times to use them would be if your processor doesn't support AVX, if it does support AVX2, or if you have a dedicated 
graphics card (the pre-built ones would just be faster in this case).

## Data

### Datasets and lexicons

Much of the data used is too large to be included in the repo, however the links to the sources are listed in the 
[dataset README](../data/datasets/README.md). Follow the links provided there. Similarly, there are lexicons that are 
not provided, they are listed in the [lexicon README](../data/lexicons/README.md).

### Preparing and pre-processing

In this repo the document pre-processing is broken into two stages, preparing and pre-processing.
Pre-processing applies the expected processing (ex. removes symbols, capitals, etc.) using a standardized set of functions.
However, in order to do this, most of the dataset/lexicons require some degree of modification before this can be applied.

To **prepare** the data, simply execute the [`prepare.py`](../execution/pre_processing/prepare.py) script.
To **pre-process** the data, again, just execute the [`pre-process`](../execution/pre_processing/pre_process.py) script.

### Before executing

Before running code you will have to make the required folders for the model/data files.
This can be generated by running the [setup](data/setup.sh) script, 
then the [dataset-specific setup](utilities/scripts/prep_dataset_run.sh).

**NOTE:** This requires the **exact** dataset name as its only argument
